# -*- coding: utf-8 -*-
"""Task_1_Part_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16a4WDe2E9trS7yjjpyKzbsSRD8iTrx53
"""

'''

This code implements a Multinomial Naive Bayes classifier to predict labels
from a deep features dataset. It begins by loading the training and validation data,
followed by flattening the 13x786 feature matrices into single vectors for each sample.
Continuous features are discretized into bins using the KBinsDiscretizer to prepare
them for the MultinomialNB model. The classifier is trained with various alpha values
while experimenting with different training sizes, evaluating accuracy on the validation
set. Finally, the results are visualized, showing how accuracy varies with training size
for each alpha, and the best alpha value with its corresponding accuracy is printed.
'''


import numpy as np
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import KBinsDiscretizer

# Load the Deep Features Dataset (replace file paths with the correct paths)
train_data = np.load('train_feature.npz')  # Load training data
valid_data = np.load('valid_feature.npz')  # Load validation data

X_train = train_data['features']  # Deep features matrix (13x786 per sample)
y_train = train_data['label']     # Labels
X_valid = valid_data['features']
y_valid = valid_data['label']

# Check the shape of the dataset to verify the dimensions
print(f"X_train shape: {X_train.shape}")  # Expected shape: (n_samples, 13, 786)
print(f"X_valid shape: {X_valid.shape}")

# Flatten the 13x786 matrix into a single vector for each sample (shape should be (n_samples, 13 * 786))
X_train_flattened = X_train.reshape(X_train.shape[0], -1)  # Reshape to (n_samples, 13*786)
X_valid_flattened = X_valid.reshape(X_valid.shape[0], -1)

# Preprocessing: Discretizing the continuous deep features into bins for MultinomialNB
# The binning process converts continuous data to discrete data, which is necessary for MultinomialNB
discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')  # 10 bins
X_train_binned = discretizer.fit_transform(X_train_flattened)  # Fit-transform the flattened training data
X_valid_binned = discretizer.transform(X_valid_flattened)  # Transform the validation data

# Define range of alpha values for tuning
alphas = np.logspace(-3, 1, 5)  # Values from 0.001 to 10

# Train Multinomial Naive Bayes with different alpha values
train_sizes = [0.2, 0.4, 0.6, 0.8, 0.99, 1]  # Training data percentages
best_alpha = None
best_acc = 0
accuracy_per_alpha = []

for alpha in alphas:
    print(f"\nTuning for alpha = {alpha:.4f}")
    clf = MultinomialNB(alpha=alpha)
    accuracy = []

    for train_size in train_sizes:
        # Split the training data according to the specified size
        X_partial, _, y_partial, _ = train_test_split(X_train_binned, y_train, train_size=train_size, random_state=42)

        # Train the model
        clf.fit(X_partial, y_partial)

        # Predict on validation set
        y_pred = clf.predict(X_valid_binned)

        # Calculate accuracy
        acc = accuracy_score(y_valid, y_pred)
        accuracy.append(acc)

    # Record accuracy for each alpha
    accuracy_per_alpha.append(accuracy)

    # Check if this is the best alpha so far
    if max(accuracy) > best_acc:
        best_acc = max(accuracy)
        best_alpha = alpha

# Plot accuracy vs training size for each alpha
for idx, alpha in enumerate(alphas):
    plt.plot([20, 40, 60, 80, 99, 100], accuracy_per_alpha[idx], label=f"alpha = {alpha:.4f}")

plt.xlabel('Percentage of Training Data Used')
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy vs Training Data Size for Deep Features Dataset (Tuned Alpha)')
plt.legend()
plt.show()

# Print the best alpha and corresponding accuracy
print(f"\nBest alpha: {best_alpha:.4f} with max validation accuracy: {best_acc:.4f}")