# -*- coding: utf-8 -*-
"""Part_1_task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12KN29H3PnorlB1QsM5iWCucL2efxq2Tu
"""

'''
This code implements a Naive Bayes classifier to predict emoticon labels from a dataset.
It begins by loading training and validation data, converting emoticons into their
Unicode representations, and applying One-Hot Encoding for feature transformation.
Various alpha values for the Multinomial Naive Bayes model are tested alongside
different sizes of training data to evaluate their impact on accuracy.
The model is trained on subsets of the data, and predictions are made on the
validation set, calculating accuracy scores for each configuration. Finally, the
results are visualized in a plot, showing how accuracy varies with training size
and alpha value, and the accuracies are printed for reference.
'''
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
import numpy as np

# Convert each emoticon (character) to its Unicode value for a string of emoticons
def emoticons_to_unicode(emoticon_string):
    return [ord(char) for char in emoticon_string]  # Convert each emoticon to Unicode

# Load the emoticon dataset (replace file paths as needed)
train_data = pd.read_csv('/content/train_emoticon.csv')  # Load training data
valid_data = pd.read_csv('/content/valid_emoticon.csv')  # Load validation data

# Extract features (emoticons) and labels
X_train = train_data.iloc[:, :-1].squeeze()  # emoticons as features, using squeeze to convert to Series
y_train = train_data.iloc[:, -1]   # labels
X_valid = valid_data.iloc[:, :-1].squeeze()
y_valid = valid_data.iloc[:, -1]

# Apply conversion to Unicode values
X_train = X_train.apply(emoticons_to_unicode)
X_valid = X_valid.apply(emoticons_to_unicode)

# Convert DataFrame columns to list of lists
X_train = X_train.tolist()
X_valid = X_valid.tolist()

# Preprocessing: One-Hot Encoding
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # Handle unknown categories if present
X_train_encoded = encoder.fit_transform(X_train)  # Fit and transform training data
X_valid_encoded = encoder.transform(X_valid)  # Transform validation data

# Define the alpha values and training sizes to experiment with
alpha_values = [0.001, 0.01, 0.1, 1, 10]
train_sizes = [0.2, 0.4, 0.6, 0.8, 0.99]  # Using float values for training data percentages
accuracy_per_size_alpha = {size: [] for size in train_sizes}  # Dictionary to store accuracies

# Iterate through each training size
for train_size in train_sizes:
    # Split the training data according to the specified size
    X_partial, _, y_partial, _ = train_test_split(X_train_encoded, y_train, train_size=train_size, random_state=42)

    # Iterate through each alpha value
    for alpha in alpha_values:
        # Train the model with the current alpha
        clf = MultinomialNB(alpha=alpha)
        clf.fit(X_partial, y_partial)

        # Predict on validation set
        y_pred = clf.predict(X_valid_encoded)

        # Calculate accuracy
        acc = accuracy_score(y_valid, y_pred)
        accuracy_per_size_alpha[train_size].append(acc)

# Plot accuracy vs training size for each alpha value
plt.figure(figsize=(10, 6))
for alpha, accuracies in zip(alpha_values, zip(*accuracy_per_size_alpha.values())):
    plt.plot([size * 100 for size in train_sizes], accuracies, marker='o', label=f"alpha = {alpha}")

plt.xlabel('Percentage of Training Data Used')
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy vs Training Data Size for Emoticons Dataset')
plt.xticks([20, 40, 60, 80, 99])
plt.legend()
plt.grid()
plt.show()

# Print the accuracy for each training size and alpha value
for size in train_sizes:
    for alpha, acc in zip(alpha_values, accuracy_per_size_alpha[size]):
        print(f"Training size: {size * 100:.0f}%, Alpha: {alpha}, Accuracy: {acc:.4f}")