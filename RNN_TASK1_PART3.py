# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13gbyY-IOO8-mC1ma5Yj3IQXzqzhb8xCL
"""

'''
This code trains an RNN model on text sequences (digits) for binary classification.
It preprocesses the input sequences by converting each digit into integers
and padding them to a fixed length. The model consists of an embedding layer
followed by multiple RNN and Dense layers. The training is performed with different
percentages of the dataset (20%, 40%, etc.) to evaluate how the size of training data affects validation accuracy.
Finally, the validation accuracies are plotted to observe model performance based on data size.

'''


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import accuracy_score

# Load the datasets
train_data = pd.read_csv('/content/train_text_seq.csv')  # Ensure your dataset is in the right folder
valid_data = pd.read_csv('/content/valid_text_seq.csv')

# Prepare the data
X_train_full = train_data['input_str'].astype(str)
y_train_full = train_data['label'].astype(int)
X_valid = valid_data['input_str'].astype(str)
y_valid = valid_data['label'].astype(int)

# Preprocess the string sequences by converting each character into an integer (digit encoding)
def preprocess_sequences(X_data, max_len=50):
    X_sequences = [[int(digit) for digit in seq] for seq in X_data]
    return pad_sequences(X_sequences, maxlen=max_len, padding='post')

X_train_full_prep = preprocess_sequences(X_train_full)
X_valid_prep = preprocess_sequences(X_valid)

# Convert labels to categorical format for multi-class classification (0 or 1)
y_train_full_cat = to_categorical(y_train_full, num_classes=2)
y_valid_cat = to_categorical(y_valid, num_classes=2)

# Function to build the RNN model with more layers
def build_rnn_model(input_length, num_rnn_layers=1):
    model = Sequential()
    model.add(Embedding(input_dim=10, output_dim=16, input_length=input_length))  # 10 unique digits (0-9)

    # Add multiple RNN layers if specified
    for _ in range(num_rnn_layers):
        model.add(SimpleRNN(units=32, activation='relu', return_sequences=True if _ < num_rnn_layers - 1 else False))

    # Add Dense layers for classification
    model.add(Dense(units=32, activation='relu'))  # Additional dense layer
    model.add(Dense(units=2, activation='softmax'))  # Output layer with 2 classes

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Function to train model with different amounts of training data
def train_rnn_and_evaluate(percentages, X_train_full, y_train_full, X_valid, y_valid, num_epochs=15, num_rnn_layers=1):
    accuracies = []
    input_length = X_train_full.shape[1]

    for perc in percentages:
        # Split the training set into the desired size (20%, 40%, etc.)
        num_examples = int(perc * len(X_train_full))
        X_train_subset = X_train_full[:num_examples]
        y_train_subset = y_train_full[:num_examples]

        # Build and train the RNN model
        model = build_rnn_model(input_length, num_rnn_layers)
        model.fit(X_train_subset, y_train_subset, epochs=num_epochs, batch_size=32, verbose=2, validation_data=(X_valid, y_valid))

        # Make predictions on validation set
        y_pred = model.predict(X_valid)
        y_pred_classes = np.argmax(y_pred, axis=1)  # Convert one-hot to class indices

        # Calculate validation accuracy
        accuracy = accuracy_score(np.argmax(y_valid, axis=1), y_pred_classes)
        accuracies.append(accuracy)
        print(f'Training with {perc * 100}% data - Validation Accuracy: {accuracy:.4f}')

    return accuracies

# Function to calculate the number of trainable parameters in the model
def calculate_trainable_params(input_dim, output_dim, rnn_units, num_rnn_layers, dense_units, input_length):
    # Embedding layer trainable parameters
    embedding_params = input_dim * output_dim

    # RNN layers trainable parameters
    rnn_params_total = 0
    rnn_input_dim = output_dim  # First RNN takes input from embedding
    for _ in range(num_rnn_layers):
        # For each RNN layer: input_dim * units + units * units + units (bias)
        rnn_params = rnn_input_dim * rnn_units + rnn_units * rnn_units + rnn_units
        rnn_params_total += rnn_params
        rnn_input_dim = rnn_units  # Subsequent layers take input from the previous RNN's output

    # Dense layers trainable parameters
    dense_params = rnn_units * dense_units + dense_units  # First Dense layer
    output_params = dense_units * 2 + 2  # Output Dense layer

    # Total parameters
    total_params = embedding_params + rnn_params_total + dense_params + output_params

    return {
        'Embedding': embedding_params,
        'RNN Layers': rnn_params_total,
        'Dense Layer': dense_params,
        'Output Layer': output_params,
        'Total': total_params
    }


# Train the RNN model with varying sizes of training data, specified epochs, and more RNN layers
percentages = [0.2, 0.4, 0.6, 0.8, 1.0]
num_epochs = 15  # Change the number of epochs
num_rnn_layers = 2  # Change the number of RNN layers (can be 1, 2, etc.)
accuracies = train_rnn_and_evaluate(percentages, X_train_full_prep, y_train_full_cat, X_valid_prep, y_valid_cat, num_epochs, num_rnn_layers)

# Plot accuracy vs training data size
plt.plot([p * 100 for p in percentages], accuracies, marker='o')
plt.xlabel('Percentage of Training Data')
plt.ylabel('Validation Accuracy')
plt.title(f'Validation Accuracy vs Training Data Size (RNN with {num_epochs} Epochs and {num_rnn_layers} RNN Layers)')
plt.grid(True)
plt.show()